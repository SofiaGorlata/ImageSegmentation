{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.datasets import make_classification\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot= True)\n",
    "from random import shuffle\n",
    "\n",
    "import glob\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TRAIN_SIZE(num):\n",
    "    print ('Total Training Images in Dataset = ' + str(mnist.train.images.shape))\n",
    "    print ('--------------------------------------------------')\n",
    "    x_train = mnist.train.images[:num,:]\n",
    "    print ('x_train Examples Loaded = ' + str(x_train.shape))\n",
    "    y_train = mnist.train.labels[:num,:]\n",
    "    print ('y_train Examples Loaded = ' + str(y_train.shape))\n",
    "    print('')\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Images in Dataset = (55000, 784)\n",
      "--------------------------------------------------\n",
      "x_train Examples Loaded = (500, 784)\n",
      "y_train Examples Loaded = (500, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = TRAIN_SIZE(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TEST_SIZE(num):\n",
    "    print ('Total Test Examples in Dataset = ' + str(mnist.test.images.shape))\n",
    "    print ('--------------------------------------------------')\n",
    "    x_test = mnist.test.images[:num,:]\n",
    "    print ('x_test Examples Loaded = ' + str(x_test.shape))\n",
    "    y_test = mnist.test.labels[:num,:]\n",
    "    print ('y_test Examples Loaded = ' + str(y_test.shape))\n",
    "    return x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiplyGate:\n",
    "    def forward(self,W, X):\n",
    "        \n",
    "        return np.dot(X, W)\n",
    "\n",
    "    def backward(self, W, X, dZ):\n",
    "        dW = np.dot(np.transpose(X), dZ)\n",
    "        dX = np.dot(dZ, np.transpose(W))\n",
    "        return dW, dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AddGate:\n",
    "    def forward(self, X, b):\n",
    "        return X + b\n",
    "\n",
    "    def backward(self, X, b, dZ):\n",
    "        dX = dZ * np.ones_like(X)\n",
    "        db = np.dot(np.ones((1, dZ.shape[0]), dtype=np.float64), dZ)\n",
    "        return db, dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self, X):\n",
    "        return 1.0 / (1.0 + np.exp(-X))\n",
    "\n",
    "    def backward(self, X, top_diff):\n",
    "        output = self.forward(X)\n",
    "        return (1.0 - output) * output * top_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, X):\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def backward(self, X, top_diff):\n",
    "        output = self.forward(X)\n",
    "        return (1.0 - np.square(output)) * top_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def predict(self, X):\n",
    "        exp_scores = np.exp(X)\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        num_examples = X.shape[0]\n",
    "        probs = self.predict(X)\n",
    "        corect_logprobs = -np.log(probs[range(num_examples), y])\n",
    "        data_loss = np.sum(corect_logprobs)\n",
    "        return 1./num_examples * data_loss\n",
    "\n",
    "    def diff(self, X, y):\n",
    "        num_examples = X.shape[0]\n",
    "        probs = self.predict(X)\n",
    "        probs[range(num_examples), y] -= 1\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layers_dim):\n",
    "        self.b = []\n",
    "        self.W = []\n",
    "        for i in range(len(layers_dim)-1):\n",
    "            self.W.append(np.random.randn(layers_dim[i], layers_dim[i+1]) / np.sqrt(layers_dim[i]))\n",
    "            self.b.append(np.random.randn(layers_dim[i+1]).reshape(1, layers_dim[i+1]))\n",
    "\n",
    "    def calculate_loss(self, X, y):\n",
    "        mulGate = MultiplyGate()\n",
    "        addGate = AddGate()\n",
    "        layer = Tanh()\n",
    "        softmaxOutput = Softmax()\n",
    "\n",
    "        input = X\n",
    "        for i in range(len(self.W)):\n",
    "            mul = mulGate.forward(self.W[i], input)\n",
    "            add = addGate.forward(mul, self.b[i])\n",
    "            input = layer.forward(add)\n",
    "\n",
    "        return softmaxOutput.loss(input, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        mulGate = MultiplyGate()\n",
    "        addGate = AddGate()\n",
    "        layer = Tanh()\n",
    "        softmaxOutput = Softmax()\n",
    "\n",
    "        input1 = X\n",
    "        for i in range(len(self.W)):\n",
    "            mul = mulGate.forward(self.W[i], input1)\n",
    "            add = addGate.forward(mul, self.b[i])\n",
    "            input1 = layer.forward(add)\n",
    "\n",
    "        probs = softmaxOutput.predict(input1)\n",
    "        print(probs)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def train(self, X, y, mini_batch_size=40, num_passes=20000, epsilon=0.01, reg_lambda=0.01, print_loss=False):\n",
    "        mulGate = MultiplyGate()\n",
    "        addGate = AddGate()\n",
    "        layer = Tanh()\n",
    "        softmaxOutput = Softmax()\n",
    "\n",
    "        #costs=[]\n",
    "        D = [0,0]\n",
    "        S = [0,0]\n",
    "        alpha = 0.999\n",
    "        beta = 0.9\n",
    "\n",
    "        \n",
    "        plotx=[]\n",
    "        ploty=[]\n",
    "        for epoch in range(num_passes):\n",
    "            # Forward propagation\n",
    "            \n",
    "            X,y=shuffle_in_unison(X,y)\n",
    "            mini_X = [\n",
    "                X[k:k+mini_batch_size]\n",
    "                for k in range(0, len(y), mini_batch_size)]\n",
    "            mini_y = [\n",
    "                y[k:k+mini_batch_size]\n",
    "                for k in range(0, len(y), mini_batch_size)]\n",
    "            \n",
    "            for ms in range(len(mini_y)):\n",
    "                input = mini_X[ms]\n",
    "                forward = [(None, None, input)]\n",
    "                for i in range(len(self.W)):\n",
    "                    mul = mulGate.forward(self.W[i], input)\n",
    "                    add = addGate.forward(mul, self.b[i])\n",
    "                    input = layer.forward(add)\n",
    "                    forward.append((mul, add, input))\n",
    "\n",
    "                # Back propagation\n",
    "                dtanh = softmaxOutput.diff(forward[len(forward)-1][2], mini_y[ms])\n",
    "                for i in range(len(forward)-1, 0, -1):\n",
    "                    dadd = layer.backward(forward[i][1], dtanh)\n",
    "                    db, dmul = addGate.backward(forward[i][0], self.b[i-1], dadd)\n",
    "                    dW, dtanh = mulGate.backward(self.W[i-1], forward[i-1][2], dmul)\n",
    "                    S[i-1] = alpha*S[i-1] +(1-alpha)*np.square(dW)\n",
    "                    D[i-1] = beta*D[i-1] +np.dot((1-beta),dW)\n",
    "                    gt = (D[i-1]/(1-beta))*((1-alpha)/S[i-1])\n",
    "            \n",
    "                    # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "                    dW += reg_lambda * self.W[i-1]+gt\n",
    "                    # Gradient descent parameter update\n",
    "                    self.b[i-1] += -epsilon * db\n",
    "                    self.W[i-1] += -epsilon * dW\n",
    "\n",
    "                if print_loss and epoch % 100 == 0:\n",
    "                    plotx.append(epoch)\n",
    "                    ploty.append( self.calculate_loss(X, y))\n",
    "                if print_loss and epoch % 1000 == 0:\n",
    "                    print(\"Loss after iteration %i: %f\" %(epoch, self.calculate_loss(X, y)))\n",
    "        return plotx,ploty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    mean = np.mean(X)\n",
    "    std = (np.max(X) - np.min(X))\n",
    "    X_new =(X-mean)/std\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Images in Dataset = (55000, 784)\n",
      "--------------------------------------------------\n",
      "x_train Examples Loaded = (500, 784)\n",
      "y_train Examples Loaded = (500, 10)\n",
      "\n",
      "[7, 3, 4, 6, 1, 8, 1, 0, 9, 8, 0, 3, 1, 2, 7, 0, 2, 9, 6, 0, 1, 6, 7, 1, 9, 7, 6, 5, 5, 8, 8, 3, 4, 4, 8, 7, 3, 6, 4, 6, 6, 3, 8, 8, 9, 9, 4, 4, 0, 7, 8, 1, 0, 0, 1, 8, 5, 7, 1, 7, 5, 5, 9, 9, 4, 2, 5, 3, 7, 4, 6, 6, 0, 1, 0, 1, 2, 4, 8, 5, 3, 5, 0, 0, 6, 4, 3, 8, 3, 7, 1, 4, 3, 9, 2, 2, 0, 3, 6, 6, 7, 4, 3, 2, 2, 4, 9, 1, 0, 5, 2, 4, 8, 2, 1, 0, 8, 4, 4, 8, 0, 6, 4, 1, 4, 9, 6, 3, 1, 2, 9, 0, 1, 0, 4, 2, 9, 9, 4, 3, 8, 6, 9, 3, 0, 6, 7, 0, 3, 1, 4, 2, 3, 3, 0, 4, 2, 5, 5, 6, 3, 7, 2, 8, 5, 9, 2, 0, 1, 1, 8, 2, 9, 3, 1, 4, 1, 5, 7, 6, 4, 7, 7, 8, 3, 9, 3, 0, 5, 1, 3, 2, 0, 3, 0, 4, 0, 7, 4, 8, 8, 9, 0, 0, 1, 8, 7, 3, 9, 9, 5, 5, 9, 6, 7, 8, 2, 4, 6, 9, 8, 1, 6, 7, 9, 1, 6, 2, 0, 9, 6, 6, 2, 9, 1, 1, 2, 1, 3, 1, 5, 2, 7, 8, 0, 1, 0, 2, 8, 0, 2, 7, 3, 7, 5, 5, 1, 8, 2, 2, 6, 9, 1, 8, 7, 4, 0, 6, 0, 7, 3, 1, 0, 6, 6, 0, 9, 3, 4, 6, 7, 8, 9, 7, 3, 0, 0, 4, 0, 2, 6, 7, 5, 4, 6, 4, 8, 2, 0, 8, 7, 1, 7, 5, 1, 1, 2, 2, 7, 5, 6, 6, 7, 4, 2, 3, 9, 0, 2, 0, 9, 0, 4, 3, 4, 7, 7, 0, 3, 6, 0, 4, 3, 8, 6, 8, 1, 3, 8, 9, 0, 9, 0, 8, 0, 2, 8, 7, 8, 7, 9, 1, 7, 0, 3, 1, 4, 2, 3, 3, 8, 4, 9, 5, 6, 6, 4, 7, 0, 8, 3, 9, 8, 0, 6, 1, 8, 2, 5, 3, 2, 4, 5, 5, 6, 6, 9, 7, 3, 8, 9, 9, 3, 0, 8, 1, 3, 2, 0, 3, 2, 4, 9, 5, 9, 6, 4, 7, 3, 8, 4, 9, 3, 6, 5, 2, 7, 5, 8, 6, 2, 2, 7, 5, 5, 1, 9, 7, 1, 1, 8, 8, 3, 3, 3, 8, 2, 7, 2, 1, 5, 7, 3, 1, 4, 4, 7, 7, 7, 2, 4, 6, 5, 5, 9, 3, 5, 9, 3, 9, 8, 0, 0, 8, 0, 7, 6, 3, 0, 0, 0, 3, 7, 7, 8, 0, 8, 1, 9, 1, 2, 2, 4, 1, 1, 6, 6, 0, 0, 5, 4, 3, 9, 3, 1, 6, 7, 3, 0, 5]\n",
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.12156864  0.51764709  0.99607849  0.99215692  0.99607849  0.83529419\n",
      "  0.32156864  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.08235294  0.55686277  0.91372555  0.98823535  0.99215692  0.98823535\n",
      "  0.99215692  0.98823535  0.87450987  0.07843138  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.48235297  0.99607849  0.99215692  0.99607849  0.99215692  0.87843144\n",
      "  0.7960785   0.7960785   0.87450987  1.          0.83529419  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.7960785   0.99215692  0.98823535  0.99215692  0.83137262\n",
      "  0.07843138  0.          0.          0.2392157   0.99215692  0.98823535\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.16078432  0.95294124  0.87843144  0.7960785\n",
      "  0.71764708  0.16078432  0.59607846  0.11764707  0.          0.          1.\n",
      "  0.99215692  0.40000004  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.15686275  0.07843138\n",
      "  0.          0.          0.40000004  0.99215692  0.19607845  0.\n",
      "  0.32156864  0.99215692  0.98823535  0.07843138  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.32156864  0.83921576  0.12156864\n",
      "  0.44313729  0.91372555  0.99607849  0.91372555  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.24313727  0.40000004  0.32156864  0.16078432  0.99215692\n",
      "  0.90980399  0.99215692  0.98823535  0.91372555  0.19607845  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.59607846  0.99215692  0.99607849  0.99215692\n",
      "  0.99607849  0.99215692  0.99607849  0.91372555  0.48235297  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.59607846  0.98823535  0.99215692\n",
      "  0.98823535  0.99215692  0.98823535  0.75294125  0.19607845  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.24313727  0.71764708\n",
      "  0.7960785   0.95294124  0.99607849  0.99215692  0.24313727  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.15686275  0.67450982  0.98823535  0.7960785   0.07843138\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.08235294  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.71764708  0.99607849\n",
      "  0.43921572  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.24313727  0.7960785   0.63921571  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.2392157\n",
      "  0.99215692  0.59215689  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.08235294  0.83921576  0.75294125  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.04313726\n",
      "  0.83529419  0.99607849  0.59215689  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.40000004  0.99215692  0.59215689  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.16078432\n",
      "  0.83529419  0.98823535  0.99215692  0.43529415  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.16078432  1.          0.83529419\n",
      "  0.36078432  0.20000002  0.          0.          0.12156864  0.36078432\n",
      "  0.67843139  0.99215692  0.99607849  0.99215692  0.55686277  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.67450982  0.98823535  0.99215692  0.98823535  0.7960785   0.7960785\n",
      "  0.91372555  0.98823535  0.99215692  0.98823535  0.99215692  0.50980395\n",
      "  0.07843138  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.08235294  0.7960785   1.          0.99215692\n",
      "  0.99607849  0.99215692  0.99607849  0.99215692  0.95686281  0.7960785\n",
      "  0.32156864  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.07843138\n",
      "  0.59215689  0.59215689  0.99215692  0.67058825  0.59215689  0.59215689\n",
      "  0.15686275  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.        ]\n",
      "Loss after iteration 0: 2.624239\n",
      "Loss after iteration 0: 2.539255\n",
      "Loss after iteration 0: 2.534290\n",
      "Loss after iteration 0: 2.514795\n",
      "Loss after iteration 0: 2.454890\n",
      "Loss after iteration 0: 2.467872\n",
      "Loss after iteration 0: 2.442082\n",
      "Loss after iteration 0: 2.468278\n",
      "Loss after iteration 0: 2.523076\n",
      "Loss after iteration 0: 2.515146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 1000: 1.480802\n",
      "Loss after iteration 1000: 1.477601\n",
      "Loss after iteration 1000: 1.477823\n",
      "Loss after iteration 1000: 1.477593\n",
      "Loss after iteration 1000: 1.490543\n",
      "Loss after iteration 1000: 1.481892\n",
      "Loss after iteration 1000: 1.481543\n",
      "Loss after iteration 1000: 1.479087\n",
      "Loss after iteration 1000: 1.481895\n",
      "Loss after iteration 1000: 1.478504\n",
      "Loss after iteration 2000: 1.411122\n",
      "Loss after iteration 2000: 1.419779\n",
      "Loss after iteration 2000: 1.416790\n",
      "Loss after iteration 2000: 1.414143\n",
      "Loss after iteration 2000: 1.413856\n",
      "Loss after iteration 2000: 1.414512\n",
      "Loss after iteration 2000: 1.414547\n",
      "Loss after iteration 2000: 1.415124\n",
      "Loss after iteration 2000: 1.412356\n",
      "Loss after iteration 2000: 1.411796\n",
      "Loss after iteration 3000: 1.390845\n",
      "Loss after iteration 3000: 1.393498\n",
      "Loss after iteration 3000: 1.393469\n",
      "Loss after iteration 3000: 1.394740\n",
      "Loss after iteration 3000: 1.391898\n",
      "Loss after iteration 3000: 1.389773\n",
      "Loss after iteration 3000: 1.393943\n",
      "Loss after iteration 3000: 1.393915\n",
      "Loss after iteration 3000: 1.393842\n",
      "Loss after iteration 3000: 1.394004\n",
      "Loss after iteration 4000: 1.337039\n",
      "Loss after iteration 4000: 1.320014\n",
      "Loss after iteration 4000: 1.315967\n",
      "Loss after iteration 4000: 1.315720\n",
      "Loss after iteration 4000: 1.328803\n",
      "Loss after iteration 4000: 1.488603\n",
      "Loss after iteration 4000: 1.357302\n",
      "Loss after iteration 4000: 1.363644\n",
      "Loss after iteration 4000: 1.479620\n",
      "Loss after iteration 4000: 1.323729\n",
      "Loss after iteration 5000: 1.133732\n",
      "Loss after iteration 5000: 1.128150\n",
      "Loss after iteration 5000: 1.134713\n",
      "Loss after iteration 5000: 1.131970\n",
      "Loss after iteration 5000: 1.124516\n",
      "Loss after iteration 5000: 1.162313\n",
      "Loss after iteration 5000: 1.152910\n",
      "Loss after iteration 5000: 1.178905\n",
      "Loss after iteration 5000: 1.169868\n",
      "Loss after iteration 5000: 1.160788\n",
      "Loss after iteration 6000: 1.082211\n",
      "Loss after iteration 6000: 1.098219\n",
      "Loss after iteration 6000: 1.187456\n",
      "Loss after iteration 6000: 1.317423\n",
      "Loss after iteration 6000: 1.117615\n",
      "Loss after iteration 6000: 1.179480\n",
      "Loss after iteration 6000: 1.166400\n",
      "Loss after iteration 6000: 1.177049\n",
      "Loss after iteration 6000: 1.196483\n",
      "Loss after iteration 6000: 1.177881\n",
      "Loss after iteration 7000: 1.101139\n",
      "Loss after iteration 7000: 1.166269\n",
      "Loss after iteration 7000: 1.242636\n",
      "Loss after iteration 7000: 1.242259\n",
      "Loss after iteration 7000: 1.121239\n",
      "Loss after iteration 7000: 1.116444\n",
      "Loss after iteration 7000: 1.110919\n",
      "Loss after iteration 7000: 1.126917\n",
      "Loss after iteration 7000: 1.125216\n",
      "Loss after iteration 7000: 1.108577\n",
      "Loss after iteration 8000: 1.103649\n",
      "Loss after iteration 8000: 1.120081\n",
      "Loss after iteration 8000: 1.118930\n",
      "Loss after iteration 8000: 1.101674\n",
      "Loss after iteration 8000: 1.269675\n",
      "Loss after iteration 8000: 1.263465\n",
      "Loss after iteration 8000: 1.139940\n",
      "Loss after iteration 8000: 1.135435\n",
      "Loss after iteration 8000: 1.117930\n",
      "Loss after iteration 8000: 1.097460\n",
      "Loss after iteration 9000: 1.159784\n",
      "Loss after iteration 9000: 1.164170\n",
      "Loss after iteration 9000: 1.110934\n",
      "Loss after iteration 9000: 1.106412\n",
      "Loss after iteration 9000: 1.238518\n",
      "Loss after iteration 9000: 1.125293\n",
      "Loss after iteration 9000: 1.102977\n",
      "Loss after iteration 9000: 1.128961\n",
      "Loss after iteration 9000: 1.131703\n",
      "Loss after iteration 9000: 1.186495\n"
     ]
    }
   ],
   "source": [
    "layers_dim = [784, 30, 10]\n",
    "x_train,y_train = TRAIN_SIZE(500)\n",
    "model = Model(layers_dim)\n",
    "y = []\n",
    "for elem in y_train:\n",
    "    for j in range(len(elem)):\n",
    "        if elem[j] == 1:\n",
    "            y.append(j)\n",
    "print(y)\n",
    "x = normalize(x_train)\n",
    "print(x_train[1])\n",
    "px,py=model.train(x, y, mini_batch_size=50, num_passes=10000, epsilon=0.07, reg_lambda=0.005, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1e34c550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06342014  0.06342014  0.06342017 ...,  0.42916924  0.06342014\n",
      "   0.06342014]\n",
      " [ 0.06163538  0.06095069  0.06181968 ...,  0.0610431   0.06095069\n",
      "   0.06095069]\n",
      " [ 0.06078844  0.06105874  0.06078842 ...,  0.06463662  0.06078842\n",
      "   0.06078842]\n",
      " ..., \n",
      " [ 0.06073242  0.06069463  0.44074363 ...,  0.06069463  0.06069463\n",
      "   0.06069463]\n",
      " [ 0.43786423  0.0593054   0.05930541 ...,  0.0593054   0.05949918\n",
      "   0.0593054 ]\n",
      " [ 0.06095662  0.06095373  0.0620041  ...,  0.06096423  0.06095373\n",
      "   0.06095373]]\n",
      "[7, 3, 4, 6, 1, 8, 1, 0, 9, 8, 0, 3, 1, 2, 7, 0, 2, 9, 6, 0, 1, 6, 7, 1, 9, 7, 6, 5, 5, 8, 8, 3, 4, 4, 8, 7, 3, 6, 4, 6, 6, 3, 8, 8, 9, 9, 4, 4, 0, 7, 8, 1, 0, 0, 1, 8, 5, 7, 1, 7, 5, 5, 9, 9, 4, 2, 5, 3, 7, 4, 6, 6, 0, 1, 0, 1, 2, 4, 8, 5, 3, 5, 0, 0, 6, 4, 3, 8, 3, 7, 1, 4, 3, 9, 2, 2, 0, 3, 6, 6, 7, 4, 3, 2, 2, 4, 9, 1, 0, 5, 2, 4, 8, 2, 1, 0, 8, 4, 4, 8, 0, 6, 4, 1, 4, 9, 6, 3, 1, 2, 9, 0, 1, 0, 4, 2, 9, 9, 4, 3, 8, 6, 9, 3, 0, 6, 7, 0, 3, 1, 4, 2, 3, 3, 0, 4, 2, 5, 5, 6, 3, 7, 2, 8, 5, 9, 2, 0, 1, 1, 8, 2, 9, 3, 1, 4, 1, 5, 7, 6, 4, 7, 7, 8, 3, 9, 3, 0, 5, 1, 3, 2, 0, 3, 0, 4, 0, 7, 4, 8, 8, 9, 0, 0, 1, 8, 7, 3, 9, 9, 5, 5, 9, 6, 7, 8, 2, 4, 6, 9, 8, 1, 6, 7, 9, 1, 6, 2, 0, 9, 6, 6, 2, 9, 1, 1, 2, 1, 3, 1, 5, 2, 7, 8, 0, 1, 0, 2, 8, 0, 2, 7, 3, 7, 5, 5, 1, 8, 2, 2, 6, 9, 1, 8, 7, 4, 0, 6, 0, 7, 3, 1, 0, 6, 6, 0, 9, 3, 4, 6, 7, 8, 9, 7, 3, 0, 0, 4, 0, 2, 6, 7, 5, 4, 6, 4, 8, 2, 0, 8, 7, 1, 7, 5, 1, 1, 2, 2, 7, 5, 6, 6, 7, 4, 2, 3, 9, 0, 2, 0, 9, 0, 4, 3, 4, 7, 7, 0, 3, 6, 0, 4, 3, 8, 6, 8, 1, 3, 8, 9, 0, 9, 0, 8, 0, 2, 8, 7, 8, 7, 9, 1, 7, 0, 3, 1, 4, 2, 3, 3, 8, 4, 9, 5, 6, 6, 4, 7, 0, 8, 3, 9, 8, 0, 6, 1, 8, 2, 5, 3, 2, 4, 5, 5, 6, 6, 9, 7, 3, 8, 9, 9, 3, 0, 8, 1, 3, 2, 0, 3]\n",
      "[7 3 4 6 1 5 1 0 8 8 0 3 1 2 7 0 2 1 6 0 1 6 7 1 1 7 6 5 0 8 5 3 1 4 8 7 3\n",
      " 6 4 6 6 3 5 5 4 4 4 4 5 7 8 1 0 0 1 5 5 7 1 7 5 5 7 1 4 2 3 3 7 4 6 6 0 1\n",
      " 0 1 2 4 5 5 3 5 0 0 6 4 3 8 3 7 1 4 3 4 2 2 0 3 6 6 7 4 3 2 2 4 4 1 0 8 2\n",
      " 4 8 2 1 0 8 4 4 8 0 6 4 1 1 4 0 3 1 3 1 0 1 6 4 2 3 4 4 3 8 6 2 3 0 6 7 0\n",
      " 1 1 4 2 3 3 0 4 2 5 5 6 3 7 2 8 5 2 2 0 1 1 8 2 4 3 1 4 1 5 7 6 4 7 7 8 3\n",
      " 1 3 0 5 1 3 2 3 3 6 4 0 7 4 8 5 1 0 0 1 8 7 3 1 2 5 5 1 6 7 8 1 4 6 1 8 1\n",
      " 6 7 1 1 6 2 0 7 6 6 2 2 1 1 2 1 3 1 8 2 7 8 0 1 0 2 8 0 2 7 3 7 5 5 1 8 2\n",
      " 2 0 2 1 8 7 4 8 6 0 7 3 1 0 6 6 0 1 3 4 6 7 8 1 7 3 5 0 4 0 2 6 7 5 4 6 4\n",
      " 8 2 4 8 7 1 7 5 1 1 2 4 7 5 0 6 7 4 2 3 7 0 2 0 7 0 4 3 1 7 7 0 3 6 0 4 3\n",
      " 8 6 8 1 3 8 1 0 1 0 8 0 2 5 7 3 7 1 1 7 0 3 1 4 2 3 3 5 4 1 5 6 6 4 7 0 8\n",
      " 3 4 8 0 6 1 5 2 5 3 2 4 5 8 6 6 2 7 3 8 7 2 3 6 8 1 2 2 0 3]\n",
      "0.8175\n",
      "[[44  0  0  1  1  2  3  0  1  0]\n",
      " [ 0 41  0  0  0  0  0  0  0  0]\n",
      " [ 0  1 35  1  1  0  0  0  0  0]\n",
      " [ 0  1  1 42  0  0  0  0  0  0]\n",
      " [ 0  3  0  0 37  0  0  0  0  0]\n",
      " [ 1  0  0  1  0 21  0  0  3  0]\n",
      " [ 3  0  0  0  0  0 37  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 39  0  0]\n",
      " [ 0  0  0  1  0 10  0  0 31  0]\n",
      " [ 0 16  7  1  8  0  0  5  1  0]]\n"
     ]
    }
   ],
   "source": [
    "plt.plot(px,py)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "newy=model.predict(x)\n",
    "print(y)\n",
    "print(newy)\n",
    "print(accuracy_score(y, newy))\n",
    "print(confusion_matrix(y, newy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Images in Dataset = (55000, 784)\n",
      "--------------------------------------------------\n",
      "x_train Examples Loaded = (200, 784)\n",
      "y_train Examples Loaded = (200, 10)\n",
      "\n",
      "[7, 3, 4, 6, 1, 8, 1, 0, 9, 8, 0, 3, 1, 2, 7, 0, 2, 9, 6, 0, 1, 6, 7, 1, 9, 7, 6, 5, 5, 8, 8, 3, 4, 4, 8, 7, 3, 6, 4, 6, 6, 3, 8, 8, 9, 9, 4, 4, 0, 7, 8, 1, 0, 0, 1, 8, 5, 7, 1, 7, 5, 5, 9, 9, 4, 2, 5, 3, 7, 4, 6, 6, 0, 1, 0, 1, 2, 4, 8, 5, 3, 5, 0, 0, 6, 4, 3, 8, 3, 7, 1, 4, 3, 9, 2, 2, 0, 3, 6, 6, 7, 4, 3, 2, 2, 4, 9, 1, 0, 5, 2, 4, 8, 2, 1, 0, 8, 4, 4, 8, 0, 6, 4, 1, 4, 9, 6, 3, 1, 2, 9, 0, 1, 0, 4, 2, 9, 9, 4, 3, 8, 6, 9, 3, 0, 6, 7, 0, 3, 1, 4, 2, 3, 3, 0, 4, 2, 5, 5, 6, 3, 7, 2, 8, 5, 9, 2, 0, 1, 1, 8, 2, 9, 3, 1, 4, 1, 5, 7, 6, 4, 7, 7, 8, 3, 9, 3, 0, 5, 1, 3, 2, 0, 3, 0, 4, 0, 7, 4, 8]\n",
      "[[ 0.06342014  0.06342014  0.06342017 ...,  0.42916924  0.06342014\n",
      "   0.06342014]\n",
      " [ 0.06163755  0.06095056  0.06181687 ...,  0.06104347  0.06095056\n",
      "   0.06095056]\n",
      " [ 0.06078832  0.06105847  0.06078831 ...,  0.06463884  0.06078831\n",
      "   0.06078831]\n",
      " ..., \n",
      " [ 0.06131104  0.06131105  0.06131109 ...,  0.44786967  0.06131104\n",
      "   0.06131104]\n",
      " [ 0.06084442  0.06181468  0.06084441 ...,  0.0608766   0.06084441\n",
      "   0.06084441]\n",
      " [ 0.06101129  0.06108433  0.06101892 ...,  0.06101105  0.45081399\n",
      "   0.06101105]]\n",
      "[7, 3, 4, 6, 1, 8, 1, 0, 9, 8, 0, 3, 1, 2, 7, 0, 2, 9, 6, 0, 1, 6, 7, 1, 9, 7, 6, 5, 5, 8, 8, 3, 4, 4, 8, 7, 3, 6, 4, 6, 6, 3, 8, 8, 9, 9, 4, 4, 0, 7, 8, 1, 0, 0, 1, 8, 5, 7, 1, 7, 5, 5, 9, 9, 4, 2, 5, 3, 7, 4, 6, 6, 0, 1, 0, 1, 2, 4, 8, 5, 3, 5, 0, 0, 6, 4, 3, 8, 3, 7, 1, 4, 3, 9, 2, 2, 0, 3, 6, 6, 7, 4, 3, 2, 2, 4, 9, 1, 0, 5, 2, 4, 8, 2, 1, 0, 8, 4, 4, 8, 0, 6, 4, 1, 4, 9, 6, 3, 1, 2, 9, 0, 1, 0, 4, 2, 9, 9, 4, 3, 8, 6, 9, 3, 0, 6, 7, 0, 3, 1, 4, 2, 3, 3, 0, 4, 2, 5, 5, 6, 3, 7, 2, 8, 5, 9, 2, 0, 1, 1, 8, 2, 9, 3, 1, 4, 1, 5, 7, 6, 4, 7, 7, 8, 3, 9, 3, 0, 5, 1, 3, 2, 0, 3, 0, 4, 0, 7, 4, 8]\n",
      "[7 3 4 6 1 8 1 0 8 8 0 3 1 2 7 0 2 1 6 0 1 6 7 1 1 7 6 5 0 8 5 3 1 4 8 7 3\n",
      " 6 4 6 6 3 5 5 4 4 4 4 5 7 8 1 0 0 1 5 8 7 1 7 5 5 7 1 4 2 3 3 7 4 6 6 0 1\n",
      " 0 1 2 4 5 5 3 5 0 0 6 4 8 8 3 7 1 4 3 4 2 2 0 3 6 6 7 4 3 2 2 4 4 1 0 8 2\n",
      " 4 8 2 1 0 8 4 4 8 0 6 4 1 1 4 0 3 1 3 1 0 1 6 4 2 3 4 4 3 8 6 2 3 0 6 7 0\n",
      " 1 1 4 2 3 3 0 4 2 5 5 6 3 7 2 8 5 2 2 0 1 1 8 2 4 3 1 4 1 5 7 6 4 7 7 8 3\n",
      " 1 3 0 5 1 3 2 3 3 6 4 0 7 4 8]\n",
      "0.82\n",
      "[[21  0  0  1  0  1  2  0  0  0]\n",
      " [ 0 22  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 17  1  0  0  0  0  0  0]\n",
      " [ 0  1  0 22  0  0  0  0  1  0]\n",
      " [ 0  2  0  0 24  0  0  0  0  0]\n",
      " [ 1  0  0  1  0 10  0  0  2  0]\n",
      " [ 1  0  0  0  0  0 17  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 17  0  0]\n",
      " [ 0  0  0  0  0  5  0  0 14  0]\n",
      " [ 0  5  2  1  7  0  0  1  1  0]]\n"
     ]
    }
   ],
   "source": [
    "x_test,y_test = TRAIN_SIZE(200)\n",
    "y = []\n",
    "for elem in y_test:\n",
    "    for j in range(len(elem)):\n",
    "        if elem[j] == 1:\n",
    "            y.append(j)\n",
    "print(y)\n",
    "x = normalize(x_test)\n",
    "testmodel=model.predict(x)\n",
    "print(y)\n",
    "print(testmodel)\n",
    "print(accuracy_score(y,testmodel))\n",
    "print(confusion_matrix(y, testmodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
